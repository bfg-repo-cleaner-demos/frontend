{
  "name": "knox",
  "description": "Amazon S3 client",
  "keywords": [
    "aws",
    "amazon",
    "s3"
  ],
  "version": "0.8.7",
  "author": {
    "name": "TJ Holowaychuk",
    "email": "tj@learnboost.com"
  },
  "contributors": [
    {
      "name": "TJ Holowaychuk",
      "email": "tj@learnboost.com"
    },
    {
      "name": "Domenic Denicola",
      "email": "domenic@domenicdenicola.com"
    },
    {
      "name": "Oleg Slobodksoi",
      "email": "oleg008@gmail.com"
    }
  ],
  "license": "MIT",
  "main": "./lib/index.js",
  "repository": {
    "type": "git",
    "url": "git://github.com/LearnBoost/knox.git"
  },
  "bugs": {
    "url": "https://github.com/LearnBoost/knox/issues"
  },
  "dependencies": {
    "mime": "*",
    "xml2js": "0.2.x",
    "debug": "~0.7.0",
    "stream-counter": "~0.1.0"
  },
  "devDependencies": {
    "mocha": "*"
  },
  "scripts": {
    "test": "mocha"
  },
  "directories": {
    "lib": "./lib"
  },
  "engines": {
    "node": ">= 0.8"
  },
  "readme": "# knox\r\n\r\nNode Amazon S3 Client.\r\n\r\n## Features\r\n\r\n  - Familiar API (`client.get()`, `client.put()`, etc.)\r\n  - Very Node-like low-level request capabilities via `http.Client`\r\n  - Higher-level API with `client.putStream()`, `client.getFile()`, etc.\r\n  - Copying and multi-file delete support\r\n  - Streaming file upload and direct stream-piping support\r\n\r\n## Examples\r\n\r\nThe following examples demonstrate some capabilities of knox and the S3 REST\r\nAPI. First things first, create an S3 client:\r\n\r\n```js\r\nvar client = knox.createClient({\r\n    key: '<api-key-here>'\r\n  , secret: '<secret-here>'\r\n  , bucket: 'learnboost'\r\n});\r\n```\r\n\r\nMore options are documented below for features like other endpoints or regions.\r\n\r\n### PUT\r\n\r\nIf you want to directly upload some strings to S3, you can use the `Client#put`\r\nmethod with a string or buffer, just like you would for any `http.Client`\r\nrequest. You pass in the filename as the first parameter, some headers for the\r\nsecond, and then listen for a `'response'` event on the request. Then send the\r\nrequest using `req.end()`. If we get a 200 response, great!\r\n\r\n```js\r\nvar object = { foo: \"bar\" };\r\nvar string = JSON.stringify(object);\r\nvar req = client.put('/test/obj.json', {\r\n    'Content-Length': string.length\r\n  , 'Content-Type': 'application/json'\r\n});\r\nreq.on('response', function(res){\r\n  if (200 == res.statusCode) {\r\n    console.log('saved to %s', req.url);\r\n  }\r\n});\r\nreq.end(string);\r\n```\r\n\r\nBy default the _x-amz-acl_ header is _private_. To alter this simply pass this\r\nheader to the client request method.\r\n\r\n```js\r\nclient.put('/test/obj.json', { 'x-amz-acl': 'public-read' });\r\n```\r\n\r\nEach HTTP verb has an alternate method with the \"File\" suffix, for example\r\n`put()` also has a higher level method named `putFile()`, accepting a source\r\nfilename and performing the dirty work shown above for you. Here is an example\r\nusage:\r\n\r\n```js\r\nclient.putFile('my.json', '/user.json', function(err, res){\r\n  // Always either do something with `res` or at least call `res.resume()`.\r\n});\r\n```\r\n\r\nAnother alternative is to stream via `Client#putStream()`, for example:\r\n\r\n```js\r\nhttp.get('http://google.com/doodle.png', function(res){\r\n  var headers = {\r\n      'Content-Length': res.headers['content-length']\r\n    , 'Content-Type': res.headers['content-type']\r\n  };\r\n  client.putStream(res, '/doodle.png', headers, function(err, res){\r\n    // check `err`, then do `res.pipe(..)` or `res.resume()` or whatever.\r\n  });\r\n});\r\n```\r\n\r\nYou can also use your stream's `pipe` method to pipe to the PUT request, but\r\nyou'll still have to set the `'Content-Length'` header. For example:\r\n\r\n```js\r\nfs.stat('./Readme.md', function(err, stat){\r\n  // Be sure to handle `err`.\r\n\r\n  var req = client.put('/Readme.md', {\r\n      'Content-Length': stat.size\r\n    , 'Content-Type': 'text/plain'\r\n  });\r\n\r\n  fs.createReadStream('./Readme.md').pipe(req);\r\n\r\n  req.on('response', function(res){\r\n    // ...\r\n  });\r\n});\r\n```\r\n\r\nFinally, if you want a nice interface for putting a buffer or a string of data,\r\nuse `Client#putBuffer()`:\r\n\r\n```js\r\nvar buffer = new Buffer('a string of data');\r\nvar headers = {\r\n  'Content-Type': 'text/plain'\r\n};\r\nclient.putBuffer(buffer, '/string.txt', headers, function(err, res){\r\n  // ...\r\n});\r\n```\r\n\r\nNote that both `putFile` and `putStream` will stream to S3 instead of reading\r\ninto memory, which is great. And they return objects that emit `'progress'`\r\nevents too, so you can monitor how the streaming goes! The progress events have\r\nfields `written`, `total`, and `percent`.\r\n\r\n### GET\r\n\r\nBelow is an example __GET__ request on the file we just shoved at S3. It simply\r\noutputs the response status code, headers, and body.\r\n\r\n```js\r\nclient.get('/test/Readme.md').on('response', function(res){\r\n  console.log(res.statusCode);\r\n  console.log(res.headers);\r\n  res.setEncoding('utf8');\r\n  res.on('data', function(chunk){\r\n    console.log(chunk);\r\n  });\r\n}).end();\r\n```\r\n\r\nThere is also `Client#getFile()` which uses a callback pattern instead of giving\r\nyou the raw request:\r\n\r\n```js\r\nclient.getFile('/test/Readme.md', function(err, res){\r\n  // check `err`, then do `res.pipe(..)` or `res.resume()` or whatever.\r\n});\r\n```\r\n\r\n### DELETE\r\n\r\nDelete our file:\r\n\r\n```js\r\nclient.del('/test/Readme.md').on('response', function(res){\r\n  console.log(res.statusCode);\r\n  console.log(res.headers);\r\n}).end();\r\n```\r\n\r\nLikewise we also have `Client#deleteFile()` as a more concise (yet less\r\nflexible) solution:\r\n\r\n```js\r\nclient.deleteFile('/test/Readme.md', function(err, res){\r\n  // check `err`, then do `res.pipe(..)` or `res.resume()` or whatever.\r\n});\r\n```\r\n\r\n### HEAD\r\n\r\nAs you might expect we have `Client#head` and `Client#headFile`, following the\r\nsame pattern as above.\r\n\r\n### Advanced Operations\r\n\r\nKnox supports a few advanced operations. Like [copying files][copy]:\r\n\r\n```js\r\nclient.copy('/test/source.txt', '/test/dest.txt').on('response', function(res){\r\n  console.log(res.statusCode);\r\n  console.log(res.headers);\r\n}).end();\r\n\r\n// or\r\n\r\nclient.copyFile('/source.txt', '/dest.txt', function(err, res){\r\n  // ...\r\n});\r\n```\r\n\r\neven between buckets:\r\n\r\n```js\r\nclient.copyTo('/source.txt', 'dest-bucket', '/dest.txt').on('response', function(res){\r\n  // ...\r\n}).end();\r\n```\r\n\r\nand even between buckets in different regions:\r\n\r\n```js\r\nvar destOptions = { region: 'us-west-2', bucket: 'dest-bucket' };\r\nclient.copyTo('/source.txt', destOptions, '/dest.txt', function(res){\r\n  // ...\r\n}).end();\r\n```\r\n\r\nor [deleting multiple files at once][multi-delete]:\r\n\r\n```js\r\nclient.deleteMultiple(['/test/Readme.md', '/test/Readme.markdown'], function(err, res){\r\n  // ...\r\n});\r\n```\r\n\r\nor [listing all the files in your bucket][list]:\r\n\r\n```js\r\nclient.list({ prefix: 'my-prefix' }, function(err, data){\r\n  /* `data` will look roughly like:\r\n\r\n  {\r\n    Prefix: 'my-prefix',\r\n    IsTruncated: true,\r\n    MaxKeys: 1000,\r\n    Contents: [\r\n      {\r\n        Key: 'whatever'\r\n        LastModified: new Date(2012, 11, 25, 0, 0, 0),\r\n        ETag: 'whatever',\r\n        Size: 123,\r\n        Owner: 'you',\r\n        StorageClass: 'whatever'\r\n      },\r\n      â‹®\r\n    ]\r\n  }\r\n\r\n  */\r\n});\r\n```\r\n\r\nAnd you can always issue ad-hoc requests, e.g. the following to\r\n[get an object's ACL][acl]:\r\n\r\n```js\r\nclient.request('GET', '/test/Readme.md?acl').on('response', function(res){\r\n  // Read and parse the XML response.\r\n  // Everyone loves XML parsing.\r\n}).end();\r\n```\r\n\r\nFinally, you can construct HTTP or HTTPS URLs for a file like so:\r\n\r\n```js\r\nvar readmeUrl = client.http('/test/Readme.md');\r\nvar userDataUrl = client.https('/user.json');\r\n```\r\n\r\n[copy]: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html\r\n[multi-delete]: http://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html\r\n[list]: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTBucketGET.html\r\n[acl]: http://docs.amazonwebservices.com/AmazonS3/latest/API/RESTObjectGETacl.html\r\n\r\n## Client Creation Options\r\n\r\nBesides the required `key`, `secret`, and `bucket` options, you can supply any\r\nof the following:\r\n\r\n### `endpoint`\r\n\r\nBy default knox will send all requests to the global endpoint\r\n(s3.amazonaws.com). This works regardless of the region where the bucket\r\nis. But if you want to manually set the endpoint, e.g. for performance or\r\ntesting reasons, or because you are using a S3-compatible service that isn't\r\nhosted by Amazon, you can do it with the `endpoint` option.\r\n\r\n### `region`\r\n\r\nFor your convenience when using buckets not in the US Standard region, you can\r\nspecify the `region` option. When you do so, the `endpoint` is automatically\r\nassembled.\r\n\r\nAs of this writing, valid values for the `region` option are:\r\n\r\n* US Standard (default): `us-standard`\r\n* US West (Oregon): `us-west-2`\r\n* US West (Northern California): `us-west-1`\r\n* EU (Ireland): `eu-west-1`\r\n* Asia Pacific (Singapore): `ap-southeast-1`\r\n* Asia Pacific (Tokyo): `ap-northeast-1`\r\n* South America (Sao Paulo): `sa-east-1`\r\n\r\nIf new regions are added later, their subdomain names will also work when passed\r\nas the `region` option. See the [AWS endpoint documentation][endpoint-docs] for\r\nthe latest list.\r\n\r\n**Convenience APIs such as `putFile` and `putStream` currently do not work as\r\nexpected with buckets in regions other than US Standard without explicitly\r\nspecify the region option.** This will eventually be addressed by resolving\r\n[issue #66][]; however, for performance reasons, it is always best to specify\r\nthe region option anyway.\r\n\r\n[endpoint-docs]: http://docs.amazonwebservices.com/general/latest/gr/rande.html#s3_region\r\n[issue #66]: https://github.com/LearnBoost/knox/issues/66\r\n\r\n### `secure` and `port`\r\n\r\nBy default, knox uses HTTPS to connect to S3 on port 443. You can override\r\neither of these with the `secure` and `port` options. Note that if you specify a\r\ncustom `port` option, the default for `secure` switches to `false`, although\r\nyou can override it manually if you want to run HTTPS against a specific port.\r\n\r\n### `token`\r\n\r\nIf you are using the [AWS Security Token Service][sts] APIs, you can construct\r\nthe client with a `token` parameter containing the temporary security\r\ncredentials token. This simply sets the _x-amz-security-token_ header on every\r\nrequest made by the client.\r\n\r\n[sts]: http://docs.amazonwebservices.com/STS/latest/UsingSTS/Welcome.html\r\n\r\n### `style`\r\n\r\nBy default, knox tries to use the \"virtual hosted style\" URLs for accessing S3,\r\ne.g. `bucket.s3.amazonaws.com`. If you pass in `\"path\"` as the `style` option,\r\nor pass in a `bucket` value that cannot be used with virtual hosted style URLs,\r\nknox will use \"path style\" URLs, e.g. `s3.amazonaws.com/bucket`. There are\r\ntradeoffs you should be aware of:\r\n\r\n- Virtual hosted style URLs can work with any region, without requiring it to be\r\n  explicitly specified; path style URLs cannot.\r\n- You can access programmatically-created buckets only by using virtual hosted\r\n  style URLs; path style URLs will not work.\r\n- You can access buckets with periods in their names over SSL using path style\r\n  URLs; virtual host style URLs will not work unless you turn off certificate\r\n  validation.\r\n- You can access buckets with mixed-case names only using path style URLs;\r\n  virtual host style URLs will not work.\r\n\r\nFor more information on the differences between these two types of URLs, and\r\nlimitations related to them, see the following S3 documentation pages:\r\n\r\n- [Virtual Hosting of Buckets][virtual]\r\n- [Bucket Configuration Options][config]\r\n- [Bucket Restrictions and Limitations][limits]\r\n\r\n[virtual]: http://docs.aws.amazon.com/AmazonS3/latest/dev/VirtualHosting.html\r\n[config]: http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketConfiguration.html\r\n[limits]: http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html\r\n\r\n### `agent`\r\n\r\nKnox disables the default [HTTP agent][], because it leads to lots of \"socket\r\nhang up\" errors when doing more than 5 requests at once. See [#116][] for\r\ndetails. If you want to get the default agent back, you can specify\r\n`agent: require(\"https\").globalAgent`, or use your own.\r\n\r\n[#116]: https://github.com/LearnBoost/knox/issues/116#issuecomment-15045187\r\n[HTTP agent]: http://nodejs.org/docs/latest/api/http.html#http_class_http_agent\r\n\r\n\r\n## Beyond Knox\r\n\r\n### Multipart Upload\r\n\r\nS3's [multipart upload][] is their [rather-complicated][] way of uploading large\r\nfiles. In particular, it is the only way of streaming files without knowing\r\ntheir Content-Length ahead of time.\r\n\r\nAdding the complexity of multipart upload directly to knox is not a great idea.\r\nFor example, it requires buffering at least 5 MiB of data at a time in memory,\r\nwhich you want to avoid if possible. Fortunately, [@nathanoehlman][] has created\r\nthe excellent [knox-mpu][] package to let you use multipart upload with knox if\r\nyou need it!\r\n\r\n[multipart upload]: aws.typepad.com/aws/2010/11/amazon-s3-multipart-upload.html\r\n[rather-complicated]: http://stackoverflow.com/q/8653146/3191\r\n[@nathanoehlman]: https://github.com/nathanoehlman\r\n[knox-mpu]: https://npmjs.org/package/knox-mpu\r\n\r\n### Easy Download/Upload\r\n\r\n[@superjoe30][] has created a nice library, called simply [s3][], that makes it\r\nvery easy to upload local files directly to S3, and download them back to your\r\nfilesystem. For simple cases this is often exactly what you want!\r\n\r\n[@superjoe30]: https://github.com/superjoe30\r\n[s3]: https://npmjs.org/package/s3\r\n\r\n### Uploading With Retries and Exponential Backoff\r\n\r\n[@jergason][] created [intimidate][], a library wrapping Knox to automatically retry\r\nfailed uploads with exponential backoff. This helps your app deal with intermittent\r\nconnectivity to S3 without bringing it to a ginding halt.\r\n\r\n[@jergason]: https://github.com/jergason\r\n[intimidate]: https://npmjs.org/package/intimidate\r\n\r\n### Listing and Copying Large Buckets\r\n\r\n[@goodeggs][] created [knox-copy][] to easily copy and stream keys of buckets beyond Amazon's 1000 key page size limit.\r\n\r\n[@goodeggs]: https://github.com/goodeggs\r\n[knox-copy]: https://npmjs.org/package/knox-copy\r\n\r\n\r\n[@segmentio][] created [s3-lister][] to stream a list of bucket keys using the new streams2 interface.\r\n\r\n[@segmentio]: https://github.com/segmentio\r\n[s3-lister]: https://npmjs.org/package/s3-lister\r\n\r\n## Running Tests\r\n\r\nTo run the test suite you must first have an S3 account. Then create a file named\r\n_./test/auth.json_, which contains your credentials as JSON, for example:\r\n\r\n```json\r\n{\r\n  \"key\": \"<api-key-here>\",\r\n  \"secret\": \"<secret-here>\",\r\n  \"bucket\": \"<your-bucket-name>\",\r\n  \"bucket2\": \"<another-bucket-name>\",\r\n  \"bucketUsWest2\": \"<bucket-in-us-west-2-region-here>\"\r\n}\r\n```\r\n\r\nThen install the dev dependencies and execute the test suite:\r\n\r\n```\r\n$ npm install\r\n$ npm test\r\n```\r\n",
  "readmeFilename": "Readme.md",
  "homepage": "https://github.com/LearnBoost/knox",
  "_id": "knox@0.8.7",
  "_from": "knox@0.8.x"
}
